{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = 'data/tinder_profile_data.csv'\n",
    "\n",
    "data = pd.read_csv(csv_filename)\n",
    "\n",
    "data = data.drop(columns = ['anthem','profile_pic_urls']) #Dropped these as no data was gathered \n",
    "\n",
    "empty_entries = data.name.isna()\n",
    "data = data[-empty_entries]\n",
    "duplicate_entries = data.duplicated()\n",
    "data = data[-duplicate_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>college</th>\n",
       "      <th>job</th>\n",
       "      <th>city</th>\n",
       "      <th>gender</th>\n",
       "      <th>distance</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miranda</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Concordia University</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3722.0</td>\n",
       "      <td>From BC and I need to learn French ASAP\\nInsta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Katherine</td>\n",
       "      <td>19.0</td>\n",
       "      <td>John Abbott College</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3706.0</td>\n",
       "      <td>Single and ready to mingle\\n ‚Ä¢reina del perreo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Katie</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3584.0</td>\n",
       "      <td>just a cupcake looking for a stud muffin \\n18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Megan</td>\n",
       "      <td>19.0</td>\n",
       "      <td>McMaster University</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Woman</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abby</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3616.0</td>\n",
       "      <td>Insta:abby.linton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eden</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Vanier College</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montr√©al</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3721.0</td>\n",
       "      <td>Lisa is my constant vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Angelika</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pembroke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chlo√©</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montr√©al</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3677.0</td>\n",
       "      <td>On diras qu‚Äôon s‚Äôest rencontr√© a l‚Äô√©picerie? üòè...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Laurie</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3719.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alexia</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3703.0</td>\n",
       "      <td>üá®üá¶ üáÆüáπ Insta: alexia_lebrun\\nSc: loulou_loveyou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mattie</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3660.0</td>\n",
       "      <td>Just for fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>√âmilie</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Coll√®ge Lionel Groulx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>Ig: mil.lalonde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ana</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3587.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Emily</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Carleton University</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ottawa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3566.0</td>\n",
       "      <td>CU Law üìö Elle Woods wannabe &amp; coffee addict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sara</td>\n",
       "      <td>19.0</td>\n",
       "      <td>C√©gep r√©gional de Lanaudi√®re</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3711.0</td>\n",
       "      <td>Don‚Äôt be shy, be curious.\\nApr√®s √ßa, on boira ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Heather</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Carleton University</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3589.0</td>\n",
       "      <td>CU biology\\n6‚Äô0 ü¶í ig @heatherbungay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kyra</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3726.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alicia</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Cegep De Sherbrooke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Meigan</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Barmaid</td>\n",
       "      <td>Montr√©al</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3716.0</td>\n",
       "      <td>Je cherche rien en particulier ici, mais je t‚Äô...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Grace</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3552.0</td>\n",
       "      <td>what it do baby</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name   age                       college      job      city gender  \\\n",
       "0     Miranda  19.0          Concordia University      NaN       NaN    NaN   \n",
       "2   Katherine  19.0           John Abbott College      NaN       NaN    NaN   \n",
       "3       Katie  19.0                           NaN      NaN       NaN    NaN   \n",
       "4       Megan  19.0           McMaster University      NaN       NaN  Woman   \n",
       "5        Abby  18.0                           NaN      NaN       NaN    NaN   \n",
       "6        Eden  18.0                Vanier College      NaN  Montr√©al    NaN   \n",
       "7    Angelika  18.0                           NaN      NaN  Pembroke    NaN   \n",
       "8       Chlo√©  20.0                           NaN      NaN  Montr√©al    NaN   \n",
       "9      Laurie  18.0                           NaN      NaN       NaN    NaN   \n",
       "10     Alexia  21.0                           NaN      NaN       NaN    NaN   \n",
       "11     Mattie  19.0                           NaN      NaN       NaN    NaN   \n",
       "12     √âmilie  19.0         Coll√®ge Lionel Groulx      NaN       NaN    NaN   \n",
       "13        Ana  18.0                           NaN      NaN       NaN    NaN   \n",
       "14      Emily  19.0           Carleton University      NaN    Ottawa    NaN   \n",
       "15       Sara  19.0  C√©gep r√©gional de Lanaudi√®re      NaN       NaN    NaN   \n",
       "16    Heather  21.0           Carleton University      NaN       NaN    NaN   \n",
       "17       Kyra  19.0                           NaN      NaN       NaN    NaN   \n",
       "18     Alicia  19.0           Cegep De Sherbrooke      NaN       NaN    NaN   \n",
       "19     Meigan  19.0                           NaN  Barmaid  Montr√©al    NaN   \n",
       "20      Grace  18.0                           NaN      NaN       NaN    NaN   \n",
       "\n",
       "    distance                                            details  \n",
       "0     3722.0  From BC and I need to learn French ASAP\\nInsta...  \n",
       "2     3706.0  Single and ready to mingle\\n ‚Ä¢reina del perreo...  \n",
       "3     3584.0      just a cupcake looking for a stud muffin \\n18  \n",
       "4     3613.0                                                NaN  \n",
       "5     3616.0                                 Insta:abby.linton   \n",
       "6     3721.0                          Lisa is my constant vibe   \n",
       "7     3455.0                                                NaN  \n",
       "8     3677.0  On diras qu‚Äôon s‚Äôest rencontr√© a l‚Äô√©picerie? üòè...  \n",
       "9     3719.0                                                NaN  \n",
       "10    3703.0    üá®üá¶ üáÆüáπ Insta: alexia_lebrun\\nSc: loulou_loveyou   \n",
       "11    3660.0                                      Just for fun   \n",
       "12    3700.0                                   Ig: mil.lalonde   \n",
       "13    3587.0                                                NaN  \n",
       "14    3566.0       CU Law üìö Elle Woods wannabe & coffee addict   \n",
       "15    3711.0  Don‚Äôt be shy, be curious.\\nApr√®s √ßa, on boira ...  \n",
       "16    3589.0                CU biology\\n6‚Äô0 ü¶í ig @heatherbungay  \n",
       "17    3726.0                                                NaN  \n",
       "18    3285.0                                                NaN  \n",
       "19    3716.0  Je cherche rien en particulier ici, mais je t‚Äô...  \n",
       "20    3552.0                                   what it do baby   "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.name = data.name.apply(lambda x: x.capitalize())\n",
    "data.age = data.age.apply(lambda x: int(x) if not np.isnan(x) else x)\n",
    "data.city = data.city.apply(lambda x: x[9:] if type(x) != float else x)\n",
    "data.distance = data.distance.apply(lambda x: int(x.split(' ')[0]) if type(x) != float else x)\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STATS----------\n",
      "2497 TOTAL ENTRIES GATHERED\n",
      "2493 ENTRIES HAVE AGE DATA (99.8%)\n",
      "1123 ENTRIES HAVE COLLEGE DATA (45.0%)\n",
      "311 ENTRIES HAVE JOB DATA (12.5%)\n",
      "623 ENTRIES HAVE CITY DATA (24.9%)\n",
      "391 ENTRIES HAVE GENDER DATA (15.7%)\n",
      "2487 ENTRIES HAVE DISTANCE DATA (99.6%)\n",
      "1733 ENTRIES HAVE DETAILS DATA (69.4%)\n",
      "-------------------------\n",
      "AVERAGE AGE IN DATA SET: 19\n",
      "NUMBER OF UNIQUE COLLEGES: 337\n",
      "NUMBER OF UNIQUE JOBS: 265\n",
      "NUMBER OF UNIQUE CITIES: 163\n"
     ]
    }
   ],
   "source": [
    "total = len(data)\n",
    "\n",
    "def summary(data):\n",
    "    print(10*\"-\" + \"STATS\" + 10*\"-\")\n",
    "    print(\"{} TOTAL ENTRIES GATHERED\".format(total))\n",
    "    list = [data.age, data.college, data.job, data.city, data.gender, data.distance, data.details]\n",
    "    names = ['AGE', 'COLLEGE', 'JOB', 'CITY', 'GENDER', 'DISTANCE', 'DETAILS']\n",
    "    n = 0\n",
    "    for each in list:\n",
    "        nums = total - each.isna().sum()\n",
    "        print(str(nums) + \" ENTRIES HAVE \" + names[n] + \" DATA (\" + str(round(100*nums/total,1)) + \"%)\")\n",
    "        n += 1\n",
    "\n",
    "    print(25*\"-\")\n",
    "    avg_age = data.age.mean()\n",
    "    print(\"AVERAGE AGE IN DATA SET: {}\".format(int(avg_age)))\n",
    "    unique_college = len(data.college.unique())\n",
    "    print(\"NUMBER OF UNIQUE COLLEGES: {}\".format(unique_college))\n",
    "    unique_jobs = len(data.job.unique())\n",
    "    print(\"NUMBER OF UNIQUE JOBS: {}\".format(unique_jobs))\n",
    "    unique_cities = len(data.city.unique())\n",
    "    print(\"NUMBER OF UNIQUE CITIES: {}\".format(unique_cities))\n",
    "\n",
    "summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.city.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "Montr√©al\n",
      "Pembroke\n",
      "Ottawa\n",
      "Gatineau\n",
      "Ogdensburg\n",
      "Kingston\n",
      "Vaudreuil-Dorion\n",
      "Saint-J√©r√¥me\n",
      "Laval\n",
      "Grand-M√®re\n",
      "Rivi√®re-du-Loup\n",
      "Drummondville\n",
      "Shawinigan\n",
      "Qu√©bec\n",
      "Quebec City\n",
      "Rimouski\n",
      "Grande-Vall√©e\n",
      "L√©vis\n",
      "Notre-Dame-du-Mont-Carmel\n",
      "Saint-Romuald\n",
      "Boischatel\n",
      "Trois-Rivi√®res\n",
      "Victoriaville\n",
      "Winnipeg\n",
      "Thompson\n",
      "Emerson\n",
      "Steinbach\n",
      "Regina\n",
      "Kipling\n",
      "Weyburn\n",
      "Oakville\n",
      "Minneapolis\n",
      "Niagara Falls\n",
      "Hamilton\n",
      "Toronto\n",
      "Mississauga\n",
      "Erin\n",
      "St. Catharines\n",
      "Aurora\n",
      "Vaughan\n",
      "Welland\n",
      "Georgetown\n",
      "Oshawa\n",
      "Guelph\n",
      "Arctic Bay\n",
      "Whitby\n",
      "Kelowna\n",
      "Whitchurch-Stouffville\n",
      "Halton Hills\n",
      "Burlington\n",
      "Santa Barbara\n",
      "Richmond Hill\n",
      "Cambridge\n",
      "Scugog\n",
      "◊ë◊®◊ô\n",
      "Keswick\n",
      "Calgary\n",
      "New York\n",
      "Barrie\n",
      "Ancaster\n",
      "Malibu\n",
      "St Catharines\n",
      "Niagara-on-the-Lake\n",
      "Pickering\n",
      "Buffalo\n",
      "Newmarket\n",
      "Windsor\n",
      "London\n",
      "Brampton\n",
      "Madrid\n",
      "Orangeville\n",
      "Bradford\n",
      "Bowmanville\n",
      "Saint-Simon\n",
      "Saint-Jean-sur-Richelieu\n",
      "Mascouche\n",
      "Sainte-Catherine\n",
      "Sainte-Julie\n",
      "Saint-Hyacinthe\n",
      "L'Assomption\n",
      "Carignan\n",
      "Saint-Dominique\n",
      "Terrebonne\n",
      "Rawdon\n",
      "Blainville\n",
      "Saint-Basile-le-Grand\n",
      "Boucherville\n",
      "Mercier\n",
      "Salaberry-de-Valleyfield\n",
      "Farnham\n",
      "Pointe-Calumet\n",
      "Montreal\n",
      "Saint-Sauveur\n",
      "Saint-Bruno-de-Montarville\n",
      "Richelieu\n",
      "Pr√©vost\n",
      "Boisbriand\n",
      "Sherbrooke\n",
      "Longueuil\n",
      "Saint-C√©saire\n",
      "Sainte-Anne-des-Lacs\n",
      "Ch√¢teauguay\n",
      "Dunham\n",
      "Joliette\n",
      "Repentigny\n",
      "Jonqui√®re\n",
      "Saint-Eustache\n",
      "Saint-Lin-Laurentides\n",
      "La Prairie\n",
      "Vancouver\n",
      "White Rock\n",
      "Ellensburg\n",
      "Victoria\n",
      "Port Coquitlam\n",
      "New Westminster\n",
      "Burnaby\n",
      "Maple Ridge\n",
      "Brisbane\n",
      "Mission\n",
      "Coquitlam\n",
      "Surrey\n",
      "Bellingham\n",
      "Port Moody\n",
      "Duncan\n",
      "Abbotsford\n",
      "Nanaimo\n",
      "Albuquerque\n",
      "Delta\n",
      "Cobble Hill\n",
      "Kamloops\n",
      "North Vancouver\n",
      "Langley\n",
      "Seattle\n",
      "Lynden\n",
      "Castlegar\n",
      "Richmond\n",
      "Ferndale\n",
      "Los Angeles\n",
      "North Cowichan\n",
      "Cranbrook\n",
      "Irricana\n",
      "Lethbridge\n",
      "Medicine Hat\n",
      "Edmonton\n",
      "Okotoks\n",
      "Rocanville\n",
      "Didsbury\n",
      "Airdrie\n",
      "Cochrane\n",
      "Crossfield\n",
      "Ciudad de M√©xico\n",
      "Lacombe\n",
      "Fort McMurray\n",
      "Red Deer\n",
      "Rocky View No. 44\n",
      "St. Albert\n",
      "Sherwood Park\n",
      "Leduc\n",
      "Fort Saskatchewan\n",
      "Cold Lake\n",
      "Grande Prairie\n",
      "St Albert\n"
     ]
    }
   ],
   "source": [
    "unique_cities = pd.unique(data.city)\n",
    "for each in unique_cities:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.city.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_city(series,city_list,KNN_model):\n",
    "    if type(series.city)==float and ~np.isnan(series.distance):\n",
    "        try:\n",
    "            city = np.random.choice(city_list[series.distance],1, replace=True)[0]\n",
    "        except:\n",
    "            if ~np.isnan(series.distance):\n",
    "                city = KNN_model.predict(np.array([[series.distance]]))[0]\n",
    "            else:\n",
    "                city = np.nan\n",
    "    else:\n",
    "        city = series.city\n",
    "    return city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_cities(data):\n",
    "    print(\"FILLING MISSING CITY VALUES\")\n",
    "    num = data.city.isna().sum()\n",
    "    print(\"FOUND {} MISSING VALUES\".format(num))\n",
    "    filtered = data[-data.city.isna()].copy()\n",
    "    filtered = filtered[-filtered.distance.isna()]\n",
    "    city_list = {}\n",
    "    for each in filtered.distance.unique():\n",
    "        city_list[each] = filtered[filtered.distance == each].city\n",
    "\n",
    "    #KNN for remaining values:\n",
    "    X = np.array(filtered.distance).reshape(-1,1)\n",
    "    y = np.array(filtered.city)\n",
    "    KNN_City = KNeighborsClassifier(n_neighbors=10).fit(X,y)\n",
    "    \n",
    "    data.city = data.apply(lambda x: fill_city(x,city_list,KNN_City), axis =1)\n",
    "    new_num = data.city.isna().sum()\n",
    "    print(\"{} MISSING CITY VALUES REMAIN ({}%)\".format(new_num,round(new_num/len(data),2)))\n",
    "    \n",
    "    return data.city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILLING MISSING CITY VALUES\n",
      "FOUND 1874 MISSING VALUES\n",
      "7 MISSING CITY VALUES REMAIN (0.0%)\n"
     ]
    }
   ],
   "source": [
    "cit = fill_missing_cities(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joliette'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
